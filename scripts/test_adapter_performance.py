#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€‚é…å™¨æ€§èƒ½æµ‹è¯•è„šæœ¬
Adapter performance testing script
"""

import asyncio
import time
import json
import sys
import os
from typing import Dict, Any, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.logging import get_logger
from app.services.exchanges.adapters.binance_adapter import BinanceDataAdapter
from app.services.exchanges.adapters.performance_monitor import performance_monitor
from app.services.exchanges.adapters.cache_manager import adapter_cache_manager
from app.services.exchanges.adapters.cache_monitor import cache_monitor

logger = get_logger(__name__)


class AdapterPerformanceTester:
    """é€‚é…å™¨æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.adapter = BinanceDataAdapter()
        self.test_results = {}
        
    def generate_test_data(self, count: int = 1000) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        test_data = []
        
        for i in range(count):
            # æ¨¡æ‹Ÿå¸å®‰äº¤æ˜“å¯¹æ•°æ®
            symbol = f"TEST{i}USDT"
            base_asset = f"TEST{i}"
            
            test_data.append({
                "symbol": symbol,
                "status": "TRADING",
                "baseAsset": base_asset,
                "quoteAsset": "USDT",
                "contractType": "PERPETUAL",
                "filters": [
                    {"filterType": "PRICE_FILTER", "tickSize": "0.10"},
                    {"filterType": "LOT_SIZE", "minQty": "0.001", "stepSize": "0.001"}
                ],
                "onboardDate": str(int(time.time() * 1000))
            })
        
        return test_data
    
    def generate_ticker_data(self, count: int = 100) -> List[Dict[str, Any]]:
        """ç”Ÿæˆtickeræµ‹è¯•æ•°æ®"""
        test_data = []
        
        for i in range(count):
            symbol = f"TEST{i}USDT"
            
            test_data.append({
                "symbol": symbol,
                "lastPrice": f"{50000 + i}.00",
                "bidPrice": f"{49999 + i}.00",
                "askPrice": f"{50001 + i}.00",
                "bidQty": "1.5",
                "askQty": "2.0",
                "openPrice": f"{49500 + i}.00",
                "highPrice": f"{50500 + i}.00",
                "lowPrice": f"{49000 + i}.00",
                "volume": "1000.00",
                "quoteVolume": "50000000.00",
                "closeTime": int(time.time() * 1000)
            })
        
        return test_data
    
    async def test_instruments_performance(self, data_sizes: List[int]):
        """æµ‹è¯•äº¤æ˜“å¯¹é€‚é…æ€§èƒ½"""
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•äº¤æ˜“å¯¹é€‚é…æ€§èƒ½")
        
        results = {}
        
        for size in data_sizes:
            logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®é‡: {size}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self.generate_test_data(size)
            
            # æµ‹è¯•ä¸²è¡Œå¤„ç†
            start_time = time.time()
            serial_result = self.adapter.adapt_instruments(test_data)
            serial_duration = time.time() - start_time
            
            # æµ‹è¯•æ‰¹é‡å¤„ç†
            start_time = time.time()
            batch_result = await self.adapter.adapt_instruments_batch(test_data)
            batch_duration = time.time() - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            serial_throughput = len(serial_result) / serial_duration if serial_duration > 0 else 0
            batch_throughput = len(batch_result) / batch_duration if batch_duration > 0 else 0
            
            improvement = ((serial_duration - batch_duration) / serial_duration * 100) if serial_duration > 0 else 0
            
            results[size] = {
                "serial": {
                    "duration": serial_duration,
                    "throughput": serial_throughput,
                    "result_count": len(serial_result)
                },
                "batch": {
                    "duration": batch_duration,
                    "throughput": batch_throughput,
                    "result_count": len(batch_result)
                },
                "improvement_percent": improvement
            }
            
            logger.info(
                f"âœ… æ•°æ®é‡ {size}: ä¸²è¡Œ {serial_duration:.3f}s ({serial_throughput:.1f} items/s), "
                f"æ‰¹é‡ {batch_duration:.3f}s ({batch_throughput:.1f} items/s), "
                f"æå‡ {improvement:.1f}%"
            )
        
        self.test_results["instruments"] = results
        return results
    
    async def test_cache_performance(self, data_sizes: List[int]):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•ç¼“å­˜æ€§èƒ½")
        
        results = {}
        
        for size in data_sizes:
            logger.info(f"ğŸ“Š æµ‹è¯•ç¼“å­˜æ•°æ®é‡: {size}")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self.generate_test_data(size)
            
            # æ¸…ç©ºç¼“å­˜
            adapter_cache_manager.clear()
            
            # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
            start_time = time.time()
            first_result = self.adapter.adapt_instruments(test_data)
            first_duration = time.time() - start_time
            
            # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
            start_time = time.time()
            second_result = self.adapter.adapt_instruments(test_data)
            second_duration = time.time() - start_time
            
            # è®¡ç®—ç¼“å­˜æ•ˆæœ
            cache_speedup = (first_duration / second_duration) if second_duration > 0 else 0
            cache_improvement = ((first_duration - second_duration) / first_duration * 100) if first_duration > 0 else 0
            
            results[size] = {
                "first_call": {
                    "duration": first_duration,
                    "result_count": len(first_result)
                },
                "cached_call": {
                    "duration": second_duration,
                    "result_count": len(second_result)
                },
                "cache_speedup": cache_speedup,
                "cache_improvement_percent": cache_improvement
            }
            
            logger.info(
                f"âœ… ç¼“å­˜æµ‹è¯• {size}: é¦–æ¬¡ {first_duration:.3f}s, "
                f"ç¼“å­˜ {second_duration:.3f}s, "
                f"åŠ é€Ÿ {cache_speedup:.1f}x, æå‡ {cache_improvement:.1f}%"
            )
        
        self.test_results["cache"] = results
        return results
    
    async def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ")
        
        import psutil
        import gc
        
        process = psutil.Process()
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
        large_test_data = self.generate_test_data(5000)
        
        # æ‰§è¡Œé€‚é…
        start_memory = process.memory_info().rss / 1024 / 1024
        result = await self.adapter.adapt_instruments_batch(large_test_data)
        end_memory = process.memory_info().rss / 1024 / 1024
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = adapter_cache_manager.get_stats()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        after_gc_memory = process.memory_info().rss / 1024 / 1024
        
        memory_results = {
            "initial_memory_mb": initial_memory,
            "start_memory_mb": start_memory,
            "end_memory_mb": end_memory,
            "after_gc_memory_mb": after_gc_memory,
            "memory_increase_mb": end_memory - start_memory,
            "cache_memory_mb": cache_stats.get("current_state", {}).get("memory_usage_mb", 0),
            "result_count": len(result)
        }
        
        self.test_results["memory"] = memory_results
        
        logger.info(
            f"âœ… å†…å­˜æµ‹è¯•: åˆå§‹ {initial_memory:.1f}MB, "
            f"å¤„ç†å {end_memory:.1f}MB, "
            f"å¢åŠ  {memory_results['memory_increase_mb']:.1f}MB, "
            f"ç¼“å­˜ {memory_results['cache_memory_mb']:.1f}MB"
        )
        
        return memory_results
    
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•")
        
        # å¯åŠ¨ç¼“å­˜ç›‘æ§
        await cache_monitor.start_monitoring()
        
        try:
            # æµ‹è¯•ä¸åŒæ•°æ®é‡
            data_sizes = [10, 50, 100, 500, 1000, 2000]
            
            # æµ‹è¯•äº¤æ˜“å¯¹é€‚é…æ€§èƒ½
            await self.test_instruments_performance(data_sizes)
            
            # æµ‹è¯•ç¼“å­˜æ€§èƒ½
            await self.test_cache_performance([100, 500, 1000])
            
            # æµ‹è¯•å†…å­˜ä½¿ç”¨
            await self.test_memory_usage()
            
            # è·å–æ€§èƒ½ç»Ÿè®¡
            perf_stats = performance_monitor.get_performance_stats()
            self.test_results["performance_stats"] = perf_stats
            
            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = adapter_cache_manager.get_stats()
            self.test_results["cache_stats"] = cache_stats
            
            # è·å–ç¼“å­˜å¥åº·æŠ¥å‘Š
            health_report = cache_monitor.get_health_report(1)  # æœ€è¿‘1å°æ—¶
            self.test_results["cache_health"] = health_report
            
        finally:
            # åœæ­¢ç¼“å­˜ç›‘æ§
            await cache_monitor.stop_monitoring()
        
        return self.test_results
    
    def save_results(self, filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"adapter_performance_test_{timestamp}.json"
        
        filepath = os.path.join("logs", filename)
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        logger.info("=" * 60)
        
        # äº¤æ˜“å¯¹é€‚é…æ€§èƒ½æ‘˜è¦
        if "instruments" in self.test_results:
            logger.info("ğŸ”§ äº¤æ˜“å¯¹é€‚é…æ€§èƒ½:")
            for size, result in self.test_results["instruments"].items():
                improvement = result["improvement_percent"]
                logger.info(
                    f"  æ•°æ®é‡ {size}: æ‰¹é‡å¤„ç†æå‡ {improvement:.1f}% "
                    f"({result['batch']['throughput']:.1f} items/s)"
                )
        
        # ç¼“å­˜æ€§èƒ½æ‘˜è¦
        if "cache" in self.test_results:
            logger.info("ğŸ’¾ ç¼“å­˜æ€§èƒ½:")
            for size, result in self.test_results["cache"].items():
                speedup = result["cache_speedup"]
                logger.info(f"  æ•°æ®é‡ {size}: ç¼“å­˜åŠ é€Ÿ {speedup:.1f}x")
        
        # å†…å­˜ä½¿ç”¨æ‘˜è¦
        if "memory" in self.test_results:
            memory = self.test_results["memory"]
            logger.info(
                f"ğŸ§  å†…å­˜ä½¿ç”¨: å¢åŠ  {memory['memory_increase_mb']:.1f}MB, "
                f"ç¼“å­˜å ç”¨ {memory['cache_memory_mb']:.1f}MB"
            )
        
        # æ•´ä½“æ€§èƒ½ç»Ÿè®¡
        if "performance_stats" in self.test_results:
            perf = self.test_results["performance_stats"]
            if "operations" in perf:
                logger.info("âš¡ æ•´ä½“æ€§èƒ½ç»Ÿè®¡:")
                for op_name, op_stats in perf["operations"].items():
                    if "duration_stats" in op_stats:
                        avg_duration = op_stats["duration_stats"]["avg"]
                        total_calls = op_stats["total_calls"]
                        logger.info(f"  {op_name}: å¹³å‡ {avg_duration:.3f}s, è°ƒç”¨ {total_calls} æ¬¡")
        
        logger.info("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨é€‚é…å™¨æ€§èƒ½æµ‹è¯•")
    
    tester = AdapterPerformanceTester()
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        await tester.run_comprehensive_test()
        
        # æ‰“å°æ‘˜è¦
        tester.print_summary()
        
        # ä¿å­˜ç»“æœ
        result_file = tester.save_results()
        
        logger.info("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return result_file
        
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        raise
    finally:
        # æ¸…ç†èµ„æº
        await adapter_cache_manager.shutdown()


if __name__ == "__main__":
    asyncio.run(main())