# -*- coding: utf-8 -*-
"""
å¸å®‰æ‰¹é‡è¯·æ±‚ä¼˜åŒ–å™¨
Binance Batch Request Optimizer - ä¼˜åŒ–APIè¯·æ±‚é¢‘ç‡å’Œæ‰¹é‡å¤„ç†
"""

import asyncio
from typing import Dict, Any, List, Optional, Callable, TypeVar
from datetime import datetime, timedelta
import time

from app.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar('T')

class BinanceBatchOptimizer:
    """å¸å®‰æ‰¹é‡è¯·æ±‚ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.batch_queue: List[Dict[str, Any]] = []
        self.batch_size = 50  # æ‰¹é‡å¤„ç†å¤§å°
        self.batch_timeout = 2.0  # æ‰¹é‡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.last_batch_time = time.time()
        
        # è¯·æ±‚ç¼“å­˜
        self.request_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_ttl = 60  # ç¼“å­˜TTLï¼ˆç§’ï¼‰
        
        logger.debug("ğŸš€ å¸å®‰æ‰¹é‡ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def add_to_batch(self, request_type: str, params: Dict[str, Any]) -> Optional[Any]:
        """æ·»åŠ è¯·æ±‚åˆ°æ‰¹é‡é˜Ÿåˆ—"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(request_type, params)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜ç»“æœ: {request_type}")
            return cached_result
        
        # æ·»åŠ åˆ°æ‰¹é‡é˜Ÿåˆ—
        request_item = {
            'type': request_type,
            'params': params,
            'timestamp': time.time(),
            'cache_key': cache_key
        }
        
        self.batch_queue.append(request_item)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³å¤„ç†æ‰¹é‡
        if len(self.batch_queue) >= self.batch_size:
            return await self._process_batch()
        
        # æ£€æŸ¥è¶…æ—¶
        if time.time() - self.last_batch_time > self.batch_timeout:
            return await self._process_batch()
        
        return None
    
    async def _process_batch(self) -> Optional[Any]:
        """å¤„ç†æ‰¹é‡è¯·æ±‚"""
        if not self.batch_queue:
            return None
        
        logger.info(f"ğŸ”„ å¤„ç†æ‰¹é‡è¯·æ±‚ï¼Œå…± {len(self.batch_queue)} ä¸ª")
        
        # æŒ‰ç±»å‹åˆ†ç»„è¯·æ±‚
        grouped_requests = self._group_requests_by_type()
        
        results = {}
        
        # å¤„ç†æ¯ç§ç±»å‹çš„è¯·æ±‚
        for request_type, requests in grouped_requests.items():
            try:
                if request_type == "positions":
                    result = await self._batch_process_positions(requests)
                elif request_type == "instruments":
                    result = await self._batch_process_instruments(requests)
                else:
                    # å…¶ä»–ç±»å‹çš„è¯·æ±‚
                    result = await self._batch_process_generic(request_type, requests)
                
                results[request_type] = result
                
                # ç¼“å­˜ç»“æœ
                for request in requests:
                    self._cache_result(request['cache_key'], result)
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡å¤„ç† {request_type} å¤±è´¥: {e}")
                continue
        
        # æ¸…ç©ºé˜Ÿåˆ—
        self.batch_queue.clear()
        self.last_batch_time = time.time()
        
        return results
    
    def _group_requests_by_type(self) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰ç±»å‹åˆ†ç»„è¯·æ±‚"""
        grouped = {}
        for request in self.batch_queue:
            request_type = request['type']
            if request_type not in grouped:
                grouped[request_type] = []
            grouped[request_type].append(request)
        return grouped
    
    async def _batch_process_positions(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†æŒä»“è¯·æ±‚"""
        # åˆå¹¶æ‰€æœ‰æŒä»“è¯·æ±‚ä¸ºå•ä¸ªAPIè°ƒç”¨
        logger.debug(f"ğŸ“Š æ‰¹é‡å¤„ç†æŒä»“è¯·æ±‚: {len(requests)} ä¸ª")
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„APIæœåŠ¡
        # æš‚æ—¶è¿”å›ç©ºç»“æœï¼Œé¿å…å®é™…APIè°ƒç”¨
        return []
    
    async def _batch_process_instruments(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†äº¤æ˜“å¯¹è¯·æ±‚"""
        logger.debug(f"ğŸ“Š æ‰¹é‡å¤„ç†äº¤æ˜“å¯¹è¯·æ±‚: {len(requests)} ä¸ª")
        
        # åˆå¹¶æ‰€æœ‰äº¤æ˜“å¯¹è¯·æ±‚ä¸ºå•ä¸ªAPIè°ƒç”¨
        return []
    
    async def _batch_process_generic(self, request_type: str, requests: List[Dict[str, Any]]) -> Any:
        """æ‰¹é‡å¤„ç†é€šç”¨è¯·æ±‚"""
        logger.debug(f"ğŸ“Š æ‰¹é‡å¤„ç† {request_type} è¯·æ±‚: {len(requests)} ä¸ª")
        return None
    
    def _generate_cache_key(self, request_type: str, params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json
        
        # åˆ›å»ºå‚æ•°çš„å“ˆå¸Œå€¼
        params_str = json.dumps(params, sort_keys=True)
        params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
        
        return f"{request_type}_{params_hash}"
    
    def _get_from_cache(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if cache_key in self.request_cache:
            cache_item = self.request_cache[cache_key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - cache_item['timestamp'] < self.cache_ttl:
                return cache_item['data']
            else:
                # åˆ é™¤è¿‡æœŸç¼“å­˜
                del self.request_cache[cache_key]
        
        return None
    
    def _cache_result(self, cache_key: str, data: Any) -> None:
        """ç¼“å­˜ç»“æœ"""
        self.request_cache[cache_key] = {
            'data': data,
            'timestamp': time.time()
        }
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.request_cache.clear()
        logger.debug("ğŸ—‘ï¸ æ‰¹é‡ä¼˜åŒ–å™¨ç¼“å­˜å·²æ¸…ç©º")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'queue_size': len(self.batch_queue),
            'cache_size': len(self.request_cache),
            'batch_size': self.batch_size,
            'batch_timeout': self.batch_timeout,
            'last_batch_time': self.last_batch_time
        }


# å…¨å±€æ‰¹é‡ä¼˜åŒ–å™¨å®ä¾‹
_batch_optimizer: Optional[BinanceBatchOptimizer] = None

def get_batch_optimizer() -> BinanceBatchOptimizer:
    """è·å–æ‰¹é‡ä¼˜åŒ–å™¨å®ä¾‹"""
    global _batch_optimizer
    if _batch_optimizer is None:
        _batch_optimizer = BinanceBatchOptimizer()
    return _batch_optimizer