# -*- coding: utf-8 -*-
"""
é€‚é…å™¨ç¼“å­˜ç®¡ç†å™¨
Adapter cache manager
"""

import asyncio
import hashlib
import json
import time
from typing import Dict, Any, Optional, Callable, TypeVar
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import OrderedDict
import pickle
import threading
from functools import wraps

from app.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar('T')


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    data: Any
    created_at: float
    expires_at: float
    access_count: int = 0
    last_accessed: float = 0
    data_size: int = 0
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        return time.time() > self.expires_at
    
    def is_valid(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ"""
        return not self.is_expired()
    
    def touch(self):
        """æ›´æ–°è®¿é—®æ—¶é—´"""
        self.last_accessed = time.time()
        self.access_count += 1


@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    total_entries: int = 0
    total_size: int = 0
    hit_count: int = 0
    miss_count: int = 0
    eviction_count: int = 0
    expired_count: int = 0
    
    @property
    def hit_rate(self) -> float:
        """ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.hit_count + self.miss_count
        return (self.hit_count / total * 100) if total > 0 else 0.0
    
    @property
    def miss_rate(self) -> float:
        """ç¼“å­˜æœªå‘½ä¸­ç‡"""
        return 100.0 - self.hit_rate


class AdapterCacheManager:
    """
    é€‚é…å™¨ç¼“å­˜ç®¡ç†å™¨
    Cache manager for adapter results
    """
    
    def __init__(
        self, 
        max_size: int = 1000,
        default_ttl: int = 300,  # 5åˆ†é’Ÿ
        cleanup_interval: int = 60  # 1åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
    ):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            default_ttl: é»˜è®¤TTLï¼ˆç§’ï¼‰
            cleanup_interval: æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cleanup_interval = cleanup_interval
        
        # ä½¿ç”¨OrderedDictå®ç°LRUç¼“å­˜
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._stats = CacheStats()
        self._lock = threading.RLock()
        
        # ä¸åŒæ•°æ®ç±»å‹çš„TTLé…ç½®
        self._ttl_config = {
            'instruments': 3600,    # äº¤æ˜“å¯¹ä¿¡æ¯1å°æ—¶
            'ticker': 30,           # tickeræ•°æ®30ç§’
            'funding_rate': 300,    # èµ„é‡‘è´¹ç‡5åˆ†é’Ÿ
            'position': 60,         # æŒä»“ä¿¡æ¯1åˆ†é’Ÿ
            'batch_instruments': 1800,  # æ‰¹é‡äº¤æ˜“å¯¹30åˆ†é’Ÿ
            'batch_tickers': 60,    # æ‰¹é‡ticker1åˆ†é’Ÿ
        }
        
        # æ¸…ç†ä»»åŠ¡ï¼ˆå»¶è¿Ÿå¯åŠ¨ï¼‰
        self._cleanup_task = None
        self._cleanup_started = False
        
        logger.info(f"ğŸ”§ é€‚é…å™¨ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: max_size={max_size}, default_ttl={default_ttl}s")
    
    def _start_cleanup_task(self):
        """å¯åŠ¨æ¸…ç†ä»»åŠ¡"""
        try:
            if not self._cleanup_started and (self._cleanup_task is None or self._cleanup_task.done()):
                self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
                self._cleanup_started = True
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œå»¶è¿Ÿå¯åŠ¨
            pass
    
    async def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜"""
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                self._cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
    
    def _generate_cache_key(self, prefix: str, data: Any, **kwargs) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        Generate cache key
        
        Args:
            prefix: é”®å‰ç¼€
            data: æ•°æ®
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            str: ç¼“å­˜é”®
        """
        try:
            # åˆ›å»ºåŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯çš„å­—å…¸
            key_data = {
                'prefix': prefix,
                'data_hash': self._hash_data(data),
                'kwargs': kwargs
            }
            
            # ç”ŸæˆJSONå­—ç¬¦ä¸²å¹¶è®¡ç®—å“ˆå¸Œ
            key_str = json.dumps(key_data, sort_keys=True)
            return hashlib.md5(key_str.encode()).hexdigest()
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆç¼“å­˜é”®å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çš„å­—ç¬¦ä¸²é”®
            return f"{prefix}_{hash(str(data))}_{hash(str(kwargs))}"
    
    def _hash_data(self, data: Any) -> str:
        """è®¡ç®—æ•°æ®å“ˆå¸Œ"""
        try:
            if isinstance(data, (list, dict)):
                # å¯¹äºå¤æ‚æ•°æ®ç»“æ„ï¼Œä½¿ç”¨JSONåºåˆ—åŒ–
                json_str = json.dumps(data, sort_keys=True, default=str)
                return hashlib.md5(json_str.encode()).hexdigest()
            else:
                # å¯¹äºç®€å•æ•°æ®ï¼Œç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                return hashlib.md5(str(data).encode()).hexdigest()
        except Exception:
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œä½¿ç”¨pickle
            try:
                return hashlib.md5(pickle.dumps(data)).hexdigest()
            except Exception:
                # æœ€åå›é€€åˆ°å­—ç¬¦ä¸²å“ˆå¸Œ
                return hashlib.md5(str(data).encode()).hexdigest()
    
    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜æ•°æ®
        Get cached data
        
        Args:
            key: ç¼“å­˜é”®
            
        Returns:
            Optional[Any]: ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        with self._lock:
            entry = self._cache.get(key)
            
            if entry is None:
                self._stats.miss_count += 1
                return None
            
            if entry.is_expired():
                # åˆ é™¤è¿‡æœŸæ¡ç›®
                del self._cache[key]
                self._stats.expired_count += 1
                self._stats.miss_count += 1
                return None
            
            # æ›´æ–°è®¿é—®ä¿¡æ¯å¹¶ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
            entry.touch()
            self._cache.move_to_end(key)
            self._stats.hit_count += 1
            
            logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {key[:16]}...")
            return entry.data
    
    def set(
        self, 
        key: str, 
        data: Any, 
        ttl: Optional[int] = None,
        data_type: Optional[str] = None
    ) -> bool:
        """
        è®¾ç½®ç¼“å­˜æ•°æ®
        Set cached data
        
        Args:
            key: ç¼“å­˜é”®
            data: è¦ç¼“å­˜çš„æ•°æ®
            ttl: ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneä½¿ç”¨é»˜è®¤å€¼
            data_type: æ•°æ®ç±»å‹ï¼Œç”¨äºç¡®å®šTTL
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®
        """
        try:
            with self._lock:
                # ç¡®å®šTTL
                if ttl is None:
                    ttl = self._ttl_config.get(data_type, self.default_ttl)
                
                # è®¡ç®—æ•°æ®å¤§å°
                data_size = self._estimate_size(data)
                
                # åˆ›å»ºç¼“å­˜æ¡ç›®
                now = time.time()
                entry = CacheEntry(
                    key=key,
                    data=data,
                    created_at=now,
                    expires_at=now + ttl,
                    last_accessed=now,
                    data_size=data_size
                )
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†ç©ºé—´
                if len(self._cache) >= self.max_size:
                    self._evict_lru()
                
                # æ·»åŠ åˆ°ç¼“å­˜
                self._cache[key] = entry
                self._stats.total_entries = len(self._cache)
                self._stats.total_size += data_size
                
                # å°è¯•å¯åŠ¨æ¸…ç†ä»»åŠ¡ï¼ˆå¦‚æœè¿˜æ²¡å¯åŠ¨ï¼‰
                if not self._cleanup_started:
                    self._start_cleanup_task()
                
                logger.debug(f"ğŸ’¾ ç¼“å­˜è®¾ç½®: {key[:16]}..., TTL={ttl}s, å¤§å°={data_size}å­—èŠ‚")
                return True
                
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def _evict_lru(self):
        """æ¸…ç†æœ€å°‘ä½¿ç”¨çš„ç¼“å­˜æ¡ç›®"""
        if not self._cache:
            return
        
        # ç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆOrderedDictçš„ç¬¬ä¸€ä¸ªï¼‰
        key, entry = self._cache.popitem(last=False)
        self._stats.total_size -= entry.data_size
        self._stats.eviction_count += 1
        
        logger.debug(f"ğŸ—‘ï¸ LRUæ¸…ç†: {key[:16]}...")
    
    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ¡ç›®"""
        with self._lock:
            expired_keys = []
            current_time = time.time()
            
            for key, entry in self._cache.items():
                if entry.expires_at < current_time:
                    expired_keys.append(key)
            
            for key in expired_keys:
                entry = self._cache.pop(key)
                self._stats.total_size -= entry.data_size
                self._stats.expired_count += 1
            
            if expired_keys:
                self._stats.total_entries = len(self._cache)
                logger.debug(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)} ä¸ªæ¡ç›®")
    
    def _estimate_size(self, data: Any) -> int:
        """ä¼°ç®—æ•°æ®å¤§å°"""
        try:
            if isinstance(data, str):
                return len(data.encode('utf-8'))
            elif isinstance(data, (list, tuple)):
                return sum(self._estimate_size(item) for item in data)
            elif isinstance(data, dict):
                return sum(
                    self._estimate_size(k) + self._estimate_size(v) 
                    for k, v in data.items()
                )
            elif hasattr(data, '__dict__'):
                # å¯¹äºè‡ªå®šä¹‰å¯¹è±¡ï¼Œä¼°ç®—å…¶å±æ€§å¤§å°
                return sum(
                    self._estimate_size(k) + self._estimate_size(v)
                    for k, v in data.__dict__.items()
                )
            else:
                # å¯¹äºå…¶ä»–ç±»å‹ï¼Œä½¿ç”¨pickleåºåˆ—åŒ–å¤§å°
                return len(pickle.dumps(data))
        except Exception:
            # å¦‚æœä¼°ç®—å¤±è´¥ï¼Œè¿”å›é»˜è®¤å¤§å°
            return 1024
    
    def cache_result(
        self, 
        cache_key_prefix: str, 
        ttl: Optional[int] = None,
        data_type: Optional[str] = None
    ):
        """
        ç¼“å­˜ç»“æœè£…é¥°å™¨
        Cache result decorator
        
        Args:
            cache_key_prefix: ç¼“å­˜é”®å‰ç¼€
            ttl: ç”Ÿå­˜æ—¶é—´
            data_type: æ•°æ®ç±»å‹
        """
        def decorator(func: Callable):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(cache_key_prefix, args, **kwargs)
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = await func(*args, **kwargs)
                if result is not None:
                    self.set(cache_key, result, ttl, data_type)
                
                return result
            
            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(cache_key_prefix, args, **kwargs)
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = func(*args, **kwargs)
                if result is not None:
                    self.set(cache_key, result, ttl, data_type)
                
                return result
            
            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        return decorator
    
    def invalidate(self, pattern: Optional[str] = None, data_type: Optional[str] = None):
        """
        å¤±æ•ˆç¼“å­˜
        Invalidate cache
        
        Args:
            pattern: é”®æ¨¡å¼ï¼ˆç®€å•å­—ç¬¦ä¸²åŒ¹é…ï¼‰
            data_type: æ•°æ®ç±»å‹
        """
        with self._lock:
            keys_to_remove = []
            
            for key in self._cache.keys():
                should_remove = False
                
                if pattern and pattern in key:
                    should_remove = True
                elif data_type and key.startswith(data_type):
                    should_remove = True
                elif pattern is None and data_type is None:
                    should_remove = True
                
                if should_remove:
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                entry = self._cache.pop(key)
                self._stats.total_size -= entry.data_size
            
            self._stats.total_entries = len(self._cache)
            
            if keys_to_remove:
                logger.info(f"ğŸ—‘ï¸ å¤±æ•ˆç¼“å­˜: {len(keys_to_remove)} ä¸ªæ¡ç›®")
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        Get cache statistics
        """
        with self._lock:
            return {
                "cache_stats": asdict(self._stats),
                "cache_config": {
                    "max_size": self.max_size,
                    "default_ttl": self.default_ttl,
                    "cleanup_interval": self.cleanup_interval,
                    "ttl_config": self._ttl_config
                },
                "current_state": {
                    "total_entries": len(self._cache),
                    "memory_usage_mb": self._stats.total_size / (1024 * 1024),
                    "oldest_entry": self._get_oldest_entry_info(),
                    "newest_entry": self._get_newest_entry_info()
                }
            }
    
    def _get_oldest_entry_info(self) -> Optional[Dict[str, Any]]:
        """è·å–æœ€æ—§æ¡ç›®ä¿¡æ¯"""
        if not self._cache:
            return None
        
        oldest_entry = min(self._cache.values(), key=lambda e: e.created_at)
        return {
            "key": oldest_entry.key[:16] + "...",
            "created_at": datetime.fromtimestamp(oldest_entry.created_at).isoformat(),
            "expires_at": datetime.fromtimestamp(oldest_entry.expires_at).isoformat(),
            "access_count": oldest_entry.access_count
        }
    
    def _get_newest_entry_info(self) -> Optional[Dict[str, Any]]:
        """è·å–æœ€æ–°æ¡ç›®ä¿¡æ¯"""
        if not self._cache:
            return None
        
        newest_entry = max(self._cache.values(), key=lambda e: e.created_at)
        return {
            "key": newest_entry.key[:16] + "...",
            "created_at": datetime.fromtimestamp(newest_entry.created_at).isoformat(),
            "expires_at": datetime.fromtimestamp(newest_entry.expires_at).isoformat(),
            "access_count": newest_entry.access_count
        }
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            self._stats = CacheStats()
            logger.info("ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")
    
    async def shutdown(self):
        """å…³é—­ç¼“å­˜ç®¡ç†å™¨"""
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        self.clear()
        logger.info("ğŸ”’ ç¼“å­˜ç®¡ç†å™¨å·²å…³é—­")


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
adapter_cache_manager = AdapterCacheManager()


def cache_adapter_result(
    cache_key_prefix: str, 
    ttl: Optional[int] = None,
    data_type: Optional[str] = None
):
    """
    é€‚é…å™¨ç»“æœç¼“å­˜è£…é¥°å™¨
    Adapter result cache decorator
    
    Args:
        cache_key_prefix: ç¼“å­˜é”®å‰ç¼€
        ttl: ç”Ÿå­˜æ—¶é—´
        data_type: æ•°æ®ç±»å‹
    """
    return adapter_cache_manager.cache_result(cache_key_prefix, ttl, data_type)