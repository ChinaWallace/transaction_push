# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ•°æ®å¤„ç†ä¼˜åŒ–å™¨
Batch data processing optimizer
"""

import asyncio
from typing import Dict, Any, List, Optional, Callable, TypeVar, Generic
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from dataclasses import dataclass
import time

from app.core.logging import get_logger
from app.models.unified_exchange_data import (
    UnifiedInstrument, UnifiedTicker, UnifiedFundingRate, UnifiedPosition
)

logger = get_logger(__name__)

T = TypeVar('T')
R = TypeVar('R')


@dataclass
class BatchConfig:
    """æ‰¹å¤„ç†é…ç½®"""
    batch_size: int = 100
    max_workers: int = 4
    use_process_pool: bool = False
    timeout: float = 30.0
    enable_parallel: bool = True


class BatchProcessor(Generic[T, R]):
    """
    æ‰¹é‡æ•°æ®å¤„ç†å™¨
    Batch data processor for optimizing large dataset transformations
    """
    
    def __init__(self, config: BatchConfig = None):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨
        
        Args:
            config: æ‰¹å¤„ç†é…ç½®
        """
        self.config = config or BatchConfig()
        self._thread_pool = None
        self._process_pool = None
        logger.info(f"ğŸ”§ æ‰¹å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ: batch_size={self.config.batch_size}, "
                   f"max_workers={self.config.max_workers}")
    
    async def process_batch(
        self, 
        data_list: List[T], 
        processor_func: Callable[[T], R],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[R]:
        """
        æ‰¹é‡å¤„ç†æ•°æ®
        Process data in batches
        
        Args:
            data_list: å¾…å¤„ç†çš„æ•°æ®åˆ—è¡¨
            processor_func: å¤„ç†å‡½æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            List[R]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        if not data_list:
            return []
        
        total_items = len(data_list)
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {total_items} ä¸ªæ•°æ®é¡¹")
        
        start_time = time.time()
        results = []
        
        try:
            if self.config.enable_parallel and total_items > self.config.batch_size:
                # å¹¶è¡Œæ‰¹å¤„ç†
                results = await self._parallel_batch_process(
                    data_list, processor_func, progress_callback
                )
            else:
                # ä¸²è¡Œæ‰¹å¤„ç†
                results = await self._serial_batch_process(
                    data_list, processor_func, progress_callback
                )
            
            duration = time.time() - start_time
            throughput = total_items / duration if duration > 0 else 0
            
            logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {len(results)}/{total_items} é¡¹, "
                       f"è€—æ—¶ {duration:.3f}ç§’, ååé‡ {throughput:.1f} items/sec")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            raise
    
    async def _parallel_batch_process(
        self,
        data_list: List[T],
        processor_func: Callable[[T], R],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[R]:
        """å¹¶è¡Œæ‰¹å¤„ç†"""
        total_items = len(data_list)
        batch_size = self.config.batch_size
        batches = [
            data_list[i:i + batch_size] 
            for i in range(0, total_items, batch_size)
        ]
        
        logger.debug(f"ğŸ”„ åˆ›å»º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œå¤„ç†")
        
        # åˆ›å»ºä»»åŠ¡
        tasks = []
        for i, batch in enumerate(batches):
            task = self._process_single_batch(
                batch, processor_func, i, len(batches)
            )
            tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶ç»“æœå¹¶å¤„ç†å¼‚å¸¸
        results = []
        processed_count = 0
        
        for i, batch_result in enumerate(batch_results):
            if isinstance(batch_result, Exception):
                logger.error(f"âŒ æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥: {batch_result}")
                # å¯¹å¤±è´¥çš„æ‰¹æ¬¡è¿›è¡Œä¸²è¡Œé‡è¯•
                try:
                    retry_result = await self._process_single_batch(
                        batches[i], processor_func, i, len(batches), retry=True
                    )
                    results.extend(retry_result)
                    processed_count += len(retry_result)
                except Exception as retry_error:
                    logger.error(f"âŒ æ‰¹æ¬¡ {i} é‡è¯•å¤±è´¥: {retry_error}")
                    continue
            else:
                results.extend(batch_result)
                processed_count += len(batch_result)
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(processed_count, total_items)
        
        return results
    
    async def _serial_batch_process(
        self,
        data_list: List[T],
        processor_func: Callable[[T], R],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[R]:
        """ä¸²è¡Œæ‰¹å¤„ç†"""
        total_items = len(data_list)
        batch_size = self.config.batch_size
        results = []
        processed_count = 0
        
        for i in range(0, total_items, batch_size):
            batch = data_list[i:i + batch_size]
            batch_index = i // batch_size
            total_batches = (total_items + batch_size - 1) // batch_size
            
            try:
                batch_result = await self._process_single_batch(
                    batch, processor_func, batch_index, total_batches
                )
                results.extend(batch_result)
                processed_count += len(batch_result)
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(processed_count, total_items)
                    
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_index} å¤„ç†å¤±è´¥: {e}")
                continue
        
        return results
    
    async def _process_single_batch(
        self,
        batch: List[T],
        processor_func: Callable[[T], R],
        batch_index: int,
        total_batches: int,
        retry: bool = False
    ) -> List[R]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        batch_size = len(batch)
        retry_suffix = " (é‡è¯•)" if retry else ""
        
        logger.debug(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_index + 1}/{total_batches}{retry_suffix}: {batch_size} é¡¹")
        
        start_time = time.time()
        results = []
        
        try:
            if self.config.use_process_pool and batch_size > 10:
                # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†CPUå¯†é›†å‹ä»»åŠ¡
                results = await self._process_with_process_pool(batch, processor_func)
            elif batch_size > 5:
                # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†I/Oå¯†é›†å‹ä»»åŠ¡
                results = await self._process_with_thread_pool(batch, processor_func)
            else:
                # ç›´æ¥ä¸²è¡Œå¤„ç†å°æ‰¹æ¬¡
                for item in batch:
                    try:
                        result = processor_func(item)
                        results.append(result)
                    except Exception as e:
                        logger.warning(f"âš ï¸ å¤„ç†å•é¡¹æ•°æ®å¤±è´¥: {e}")
                        continue
            
            duration = time.time() - start_time
            throughput = batch_size / duration if duration > 0 else 0
            
            logger.debug(f"âœ… æ‰¹æ¬¡ {batch_index + 1}{retry_suffix} å®Œæˆ: "
                        f"{len(results)}/{batch_size} é¡¹, "
                        f"è€—æ—¶ {duration:.3f}ç§’, ååé‡ {throughput:.1f} items/sec")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_index + 1}{retry_suffix} å¤„ç†å¼‚å¸¸: {e}")
            raise
    
    async def _process_with_thread_pool(
        self, 
        batch: List[T], 
        processor_func: Callable[[T], R]
    ) -> List[R]:
        """ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ‰¹æ¬¡"""
        if not self._thread_pool:
            self._thread_pool = ThreadPoolExecutor(max_workers=self.config.max_workers)
        
        loop = asyncio.get_event_loop()
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [
            loop.run_in_executor(self._thread_pool, processor_func, item)
            for item in batch
        ]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.warning(f"âš ï¸ çº¿ç¨‹æ± å¤„ç†é¡¹ç›® {i} å¤±è´¥: {result}")
            else:
                valid_results.append(result)
        
        return valid_results
    
    async def _process_with_process_pool(
        self, 
        batch: List[T], 
        processor_func: Callable[[T], R]
    ) -> List[R]:
        """ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†æ‰¹æ¬¡"""
        if not self._process_pool:
            self._process_pool = ProcessPoolExecutor(max_workers=self.config.max_workers)
        
        loop = asyncio.get_event_loop()
        
        try:
            # åˆ›å»ºä»»åŠ¡
            tasks = [
                loop.run_in_executor(self._process_pool, processor_func, item)
                for item in batch
            ]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=self.config.timeout
            )
            
            # è¿‡æ»¤å¼‚å¸¸ç»“æœ
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.warning(f"âš ï¸ è¿›ç¨‹æ± å¤„ç†é¡¹ç›® {i} å¤±è´¥: {result}")
                else:
                    valid_results.append(result)
            
            return valid_results
            
        except asyncio.TimeoutError:
            logger.error(f"âŒ è¿›ç¨‹æ± å¤„ç†è¶…æ—¶ ({self.config.timeout}ç§’)")
            raise
    
    def optimize_batch_size(self, data_sample: List[T], processor_func: Callable[[T], R]) -> int:
        """
        ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        Optimize batch size based on performance testing
        
        Args:
            data_sample: æ•°æ®æ ·æœ¬
            processor_func: å¤„ç†å‡½æ•°
            
        Returns:
            int: ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°
        """
        if len(data_sample) < 10:
            return len(data_sample)
        
        logger.info("ğŸ” å¼€å§‹æ‰¹æ¬¡å¤§å°ä¼˜åŒ–æµ‹è¯•")
        
        # æµ‹è¯•ä¸åŒçš„æ‰¹æ¬¡å¤§å°
        test_sizes = [10, 25, 50, 100, 200]
        test_sample = data_sample[:min(100, len(data_sample))]
        best_size = self.config.batch_size
        best_throughput = 0
        
        for size in test_sizes:
            if size > len(test_sample):
                continue
            
            try:
                start_time = time.time()
                test_batch = test_sample[:size]
                
                # æµ‹è¯•å¤„ç†æ€§èƒ½
                for item in test_batch:
                    processor_func(item)
                
                duration = time.time() - start_time
                throughput = size / duration if duration > 0 else 0
                
                logger.debug(f"ğŸ“Š æ‰¹æ¬¡å¤§å° {size}: ååé‡ {throughput:.1f} items/sec")
                
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_size = size
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æ‰¹æ¬¡å¤§å° {size} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        logger.info(f"ğŸ¯ ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°: {best_size} (ååé‡: {best_throughput:.1f} items/sec)")
        return best_size
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.cleanup()
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self._thread_pool:
            self._thread_pool.shutdown(wait=True)
            self._thread_pool = None
        
        if self._process_pool:
            self._process_pool.shutdown(wait=True)
            self._process_pool = None
        
        logger.debug("ğŸ§¹ æ‰¹å¤„ç†å™¨èµ„æºæ¸…ç†å®Œæˆ")


class AdapterBatchProcessor:
    """
    é€‚é…å™¨ä¸“ç”¨æ‰¹å¤„ç†å™¨
    Specialized batch processor for adapters
    """
    
    def __init__(self):
        self.instrument_processor = BatchProcessor[Dict[str, Any], UnifiedInstrument](
            BatchConfig(batch_size=200, max_workers=4, enable_parallel=True)
        )
        self.ticker_processor = BatchProcessor[Dict[str, Any], UnifiedTicker](
            BatchConfig(batch_size=100, max_workers=3, enable_parallel=True)
        )
        self.funding_rate_processor = BatchProcessor[Dict[str, Any], UnifiedFundingRate](
            BatchConfig(batch_size=50, max_workers=2, enable_parallel=True)
        )
        self.position_processor = BatchProcessor[Dict[str, Any], UnifiedPosition](
            BatchConfig(batch_size=50, max_workers=2, enable_parallel=True)
        )
        
        logger.info("ğŸ”§ é€‚é…å™¨æ‰¹å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def batch_adapt_instruments(
        self, 
        raw_data: List[Dict[str, Any]], 
        adapter_func: Callable[[Dict[str, Any]], UnifiedInstrument],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[UnifiedInstrument]:
        """æ‰¹é‡é€‚é…äº¤æ˜“å¯¹æ•°æ®"""
        return await self.instrument_processor.process_batch(
            raw_data, adapter_func, progress_callback
        )
    
    async def batch_adapt_tickers(
        self, 
        raw_data: List[Dict[str, Any]], 
        adapter_func: Callable[[Dict[str, Any]], UnifiedTicker],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[UnifiedTicker]:
        """æ‰¹é‡é€‚é…tickeræ•°æ®"""
        return await self.ticker_processor.process_batch(
            raw_data, adapter_func, progress_callback
        )
    
    async def batch_adapt_funding_rates(
        self, 
        raw_data: List[Dict[str, Any]], 
        adapter_func: Callable[[Dict[str, Any]], UnifiedFundingRate],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[UnifiedFundingRate]:
        """æ‰¹é‡é€‚é…èµ„é‡‘è´¹ç‡æ•°æ®"""
        return await self.funding_rate_processor.process_batch(
            raw_data, adapter_func, progress_callback
        )
    
    async def batch_adapt_positions(
        self, 
        raw_data: List[Dict[str, Any]], 
        adapter_func: Callable[[Dict[str, Any]], UnifiedPosition],
        progress_callback: Optional[Callable[[int, int], None]] = None
    ) -> List[UnifiedPosition]:
        """æ‰¹é‡é€‚é…æŒä»“æ•°æ®"""
        return await self.position_processor.process_batch(
            raw_data, adapter_func, progress_callback
        )
    
    async def cleanup(self):
        """æ¸…ç†æ‰€æœ‰å¤„ç†å™¨èµ„æº"""
        await asyncio.gather(
            self.instrument_processor.cleanup(),
            self.ticker_processor.cleanup(),
            self.funding_rate_processor.cleanup(),
            self.position_processor.cleanup(),
            return_exceptions=True
        )
        logger.info("ğŸ§¹ é€‚é…å™¨æ‰¹å¤„ç†å™¨æ¸…ç†å®Œæˆ")


# å…¨å±€æ‰¹å¤„ç†å™¨å®ä¾‹
adapter_batch_processor = AdapterBatchProcessor()