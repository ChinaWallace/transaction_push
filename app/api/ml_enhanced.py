# -*- coding: utf-8 -*-
"""
æœºå™¨å­¦ä¹ å¢å¼ºAPI
ML Enhanced API endpoints for prediction, anomaly detection and optimization
"""

from typing import List, Dict, Any, Optional
from datetime import datetime
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel, Field

from app.core.logging import get_logger
from app.core.config import get_settings
from app.core.ml_weight_config import get_ml_weight_config
from app.services.ml_enhanced_service import MLEnhancedService, PredictionSignal, AnomalyType
from app.schemas.base import BaseResponse

logger = get_logger(__name__)
settings = get_settings()

router = APIRouter()


class StrategyOverviewResponse(BaseResponse):
    """ç­–ç•¥æ¦‚è§ˆå“åº”æ¨¡å‹"""
    symbol: str
    traditional_signals: Dict[str, Any]  # ä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡
    ml_prediction: Dict[str, Any]        # MLé¢„æµ‹
    anomaly_detection: List[Dict[str, Any]]  # å¼‚å¸¸æ£€æµ‹
    combined_recommendation: str         # ç»¼åˆå»ºè®®
    confidence_score: float             # ç»¼åˆç½®ä¿¡åº¦


class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    symbol: str = Field(..., description="äº¤æ˜“å¯¹")
    include_features: bool = Field(default=False, description="æ˜¯å¦åŒ…å«ç‰¹å¾é‡è¦æ€§")


class PredictionResponse(BaseResponse):
    """é¢„æµ‹å“åº”æ¨¡å‹"""
    symbol: str
    signal: str
    confidence: float
    probability_distribution: Dict[str, float]
    features_importance: Optional[Dict[str, float]] = None
    model_accuracy: float
    timestamp: datetime


class AnomalyDetectionRequest(BaseModel):
    """å¼‚å¸¸æ£€æµ‹è¯·æ±‚æ¨¡å‹"""
    symbol: str = Field(..., description="äº¤æ˜“å¯¹")
    detection_types: Optional[List[str]] = Field(default=None, description="æ£€æµ‹ç±»å‹åˆ—è¡¨")


class AnomalyResponse(BaseModel):
    """å¼‚å¸¸å“åº”æ¨¡å‹"""
    symbol: str
    anomaly_type: str
    severity: float
    description: str
    affected_features: List[str]
    recommendation: str
    timestamp: datetime


class AnomalyDetectionResponse(BaseResponse):
    """å¼‚å¸¸æ£€æµ‹å“åº”æ¨¡å‹"""
    symbol: str
    anomalies: List[AnomalyResponse]
    total_anomalies: int
    timestamp: datetime


class OptimizationRequest(BaseModel):
    """å‚æ•°ä¼˜åŒ–è¯·æ±‚æ¨¡å‹"""
    symbol: str = Field(..., description="äº¤æ˜“å¯¹")
    optimization_type: str = Field(default="all", description="ä¼˜åŒ–ç±»å‹: all, supertrend, volume")


class OptimizationResponse(BaseResponse):
    """å‚æ•°ä¼˜åŒ–å“åº”æ¨¡å‹"""
    symbol: str
    optimized_parameters: Dict[str, Any]
    performance_improvement: Optional[float] = None
    timestamp: datetime


class BatchAnalysisRequest(BaseModel):
    """æ‰¹é‡åˆ†æè¯·æ±‚æ¨¡å‹"""
    symbols: List[str] = Field(..., description="äº¤æ˜“å¯¹åˆ—è¡¨")
    include_prediction: bool = Field(default=True, description="æ˜¯å¦åŒ…å«é¢„æµ‹")
    include_anomaly_detection: bool = Field(default=True, description="æ˜¯å¦åŒ…å«å¼‚å¸¸æ£€æµ‹")
    include_optimization: bool = Field(default=False, description="æ˜¯å¦åŒ…å«å‚æ•°ä¼˜åŒ–")


class BatchAnalysisResponse(BaseResponse):
    """æ‰¹é‡åˆ†æå“åº”æ¨¡å‹"""
    results: Dict[str, Dict[str, Any]]
    summary: Dict[str, Any]
    timestamp: datetime


# åˆ›å»ºMLæœåŠ¡å®ä¾‹
ml_service = MLEnhancedService()


@router.post("/predict", response_model=PredictionResponse)
async def predict_signal(request: PredictionRequest) -> PredictionResponse:
    """
    MLä¿¡å·é¢„æµ‹
    
    ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹äº¤æ˜“ä¿¡å·
    """
    try:
        # æ‰§è¡Œé¢„æµ‹
        prediction = await ml_service.predict_signal(request.symbol)
        
        # æ„å»ºå“åº”
        response_data = {
            "status": "success",
            "message": f"é¢„æµ‹å®Œæˆ: {prediction.signal.value}",
            "symbol": prediction.symbol,
            "signal": prediction.signal.value,
            "confidence": prediction.confidence,
            "probability_distribution": prediction.probability_distribution,
            "model_accuracy": prediction.model_accuracy,
            "timestamp": prediction.timestamp
        }
        
        if request.include_features:
            response_data["features_importance"] = prediction.features_importance
        
        return PredictionResponse(**response_data)
        
    except Exception as e:
        logger.error(f"Prediction API failed for {request.symbol}: {e}")
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")


@router.post("/anomaly-detection", response_model=AnomalyDetectionResponse)
async def detect_anomalies(request: AnomalyDetectionRequest) -> AnomalyDetectionResponse:
    """
    å¼‚å¸¸æ£€æµ‹
    
    æ£€æµ‹å¸‚åœºæ•°æ®ä¸­çš„å¼‚å¸¸æ¨¡å¼
    """
    try:
        # æ‰§è¡Œå¼‚å¸¸æ£€æµ‹
        anomalies = await ml_service.detect_anomalies(request.symbol)
        
        # è¿‡æ»¤æ£€æµ‹ç±»å‹
        if request.detection_types:
            anomalies = [
                anomaly for anomaly in anomalies
                if anomaly.anomaly_type.value in request.detection_types
            ]
        
        # æ„å»ºå¼‚å¸¸å“åº”åˆ—è¡¨
        anomaly_responses = []
        for anomaly in anomalies:
            anomaly_responses.append(AnomalyResponse(
                symbol=anomaly.symbol,
                anomaly_type=anomaly.anomaly_type.value,
                severity=anomaly.severity,
                description=anomaly.description,
                affected_features=anomaly.affected_features,
                recommendation=anomaly.recommendation,
                timestamp=anomaly.timestamp
            ))
        
        return AnomalyDetectionResponse(
            status="success",
            message=f"æ£€æµ‹åˆ° {len(anomaly_responses)} ä¸ªå¼‚å¸¸",
            symbol=request.symbol,
            anomalies=anomaly_responses,
            total_anomalies=len(anomaly_responses),
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Anomaly detection API failed for {request.symbol}: {e}")
        raise HTTPException(status_code=500, detail=f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")


@router.post("/optimize", response_model=OptimizationResponse)
async def optimize_parameters(request: OptimizationRequest) -> OptimizationResponse:
    """
    å‚æ•°ä¼˜åŒ–
    
    ä½¿ç”¨æœºå™¨å­¦ä¹ ä¼˜åŒ–ç­–ç•¥å‚æ•°
    """
    try:
        # æ‰§è¡Œå‚æ•°ä¼˜åŒ–
        optimized_params = await ml_service.optimize_parameters(request.symbol)
        
        return OptimizationResponse(
            status="success",
            message="å‚æ•°ä¼˜åŒ–å®Œæˆ",
            symbol=request.symbol,
            optimized_parameters=optimized_params,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Parameter optimization API failed for {request.symbol}: {e}")
        raise HTTPException(status_code=500, detail=f"å‚æ•°ä¼˜åŒ–å¤±è´¥: {str(e)}")


@router.post("/batch-analysis", response_model=BatchAnalysisResponse)
async def batch_analysis(request: BatchAnalysisRequest) -> BatchAnalysisResponse:
    """
    æ‰¹é‡MLåˆ†æ
    
    å¯¹å¤šä¸ªäº¤æ˜“å¯¹è¿›è¡Œæ‰¹é‡MLåˆ†æ
    """
    try:
        results = {}
        summary = {
            "total_symbols": len(request.symbols),
            "successful_predictions": 0,
            "total_anomalies": 0,
            "optimized_symbols": 0,
            "signals_distribution": {
                "strong_buy": 0,
                "buy": 0,
                "hold": 0,
                "sell": 0,
                "strong_sell": 0
            }
        }
        
        for symbol in request.symbols:
            symbol_result = {}
            
            try:
                # é¢„æµ‹åˆ†æ
                if request.include_prediction:
                    prediction = await ml_service.predict_signal(symbol)
                    symbol_result["prediction"] = {
                        "signal": prediction.signal.value,
                        "confidence": prediction.confidence,
                        "model_accuracy": prediction.model_accuracy
                    }
                    summary["successful_predictions"] += 1
                    summary["signals_distribution"][prediction.signal.value] += 1
                
                # å¼‚å¸¸æ£€æµ‹
                if request.include_anomaly_detection:
                    anomalies = await ml_service.detect_anomalies(symbol)
                    symbol_result["anomalies"] = [
                        {
                            "type": anomaly.anomaly_type.value,
                            "severity": anomaly.severity,
                            "description": anomaly.description
                        }
                        for anomaly in anomalies
                    ]
                    summary["total_anomalies"] += len(anomalies)
                
                # å‚æ•°ä¼˜åŒ–
                if request.include_optimization:
                    optimized_params = await ml_service.optimize_parameters(symbol)
                    symbol_result["optimization"] = optimized_params
                    summary["optimized_symbols"] += 1
                
                results[symbol] = symbol_result
                
            except Exception as e:
                logger.error(f"Batch analysis failed for {symbol}: {e}")
                results[symbol] = {"error": str(e)}
        
        return BatchAnalysisResponse(
            status="success",
            message=f"æ‰¹é‡åˆ†æå®Œæˆï¼Œå¤„ç†äº† {len(request.symbols)} ä¸ªäº¤æ˜“å¯¹",
            results=results,
            summary=summary,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Batch analysis API failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")


@router.get("/model-status/{symbol}")
async def get_model_status(symbol: str) -> Dict[str, Any]:
    """
    è·å–æ¨¡å‹çŠ¶æ€
    
    æŸ¥çœ‹æŒ‡å®šäº¤æ˜“å¯¹çš„MLæ¨¡å‹çŠ¶æ€
    """
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        model_exists = symbol in ml_service.prediction_models
        scaler_exists = symbol in ml_service.scalers
        
        status = {
            "symbol": symbol,
            "model_loaded": model_exists,
            "scaler_loaded": scaler_exists,
            "model_accuracy": getattr(ml_service.prediction_models.get(symbol), '_accuracy', None),
            "last_updated": None,  # å¯ä»¥ä»æ–‡ä»¶ä¿®æ”¹æ—¶é—´è·å–
            "status": "ready" if (model_exists and scaler_exists) else "not_initialized"
        }
        
        return {
            "status": "success",
            "data": status,
            "timestamp": datetime.now()
        }
        
    except Exception as e:
        logger.error(f"Model status check failed for {symbol}: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")


@router.post("/initialize-models")
async def initialize_models(background_tasks: BackgroundTasks, 
                          symbols: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    åˆå§‹åŒ–MLæ¨¡å‹
    
    ä¸ºæŒ‡å®šçš„äº¤æ˜“å¯¹åˆå§‹åŒ–æˆ–é‡æ–°è®­ç»ƒMLæ¨¡å‹
    """
    try:
        if symbols is None:
            symbols = settings.monitored_symbols
        
        # åœ¨åå°ä»»åŠ¡ä¸­åˆå§‹åŒ–æ¨¡å‹
        background_tasks.add_task(ml_service.initialize_models, symbols)
        
        return {
            "status": "success",
            "message": f"å¼€å§‹åˆå§‹åŒ– {len(symbols)} ä¸ªäº¤æ˜“å¯¹çš„MLæ¨¡å‹",
            "symbols": symbols,
            "timestamp": datetime.now()
        }
        
    except Exception as e:
        logger.error(f"Model initialization failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")


@router.get("/config")
async def get_ml_config() -> Dict[str, Any]:
    """
    è·å–MLé…ç½®
    
    è¿”å›å½“å‰çš„æœºå™¨å­¦ä¹ é…ç½®å‚æ•°
    """
    return {
        "status": "success",
        "config": settings.ml_config,
        "monitored_symbols": settings.monitored_symbols,
        "timestamp": datetime.now()
    }


@router.get("/strategy-overview/{symbol}", response_model=StrategyOverviewResponse)
async def get_strategy_overview(symbol: str) -> StrategyOverviewResponse:
    """
    ç»¼åˆç­–ç•¥æ¦‚è§ˆ
    
    ç»“åˆä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡å’Œæœºå™¨å­¦ä¹ åˆ†æï¼Œæä¾›ç»¼åˆæŠ•èµ„å»ºè®®
    """
    try:
        # å¯¼å…¥å…¶ä»–æœåŠ¡
        from app.services.trend_analysis_service import TrendAnalysisService
        from app.services.binance_service import BinanceService
        
        trend_service = TrendAnalysisService()
        binance_service = BinanceService()
        
        # è·å–ä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡åˆ†æ
        traditional_analysis = await trend_service.analyze_symbol(symbol)
        
        # è·å–MLé¢„æµ‹
        ml_prediction = await ml_service.predict_signal(symbol)
        
        # è·å–å¼‚å¸¸æ£€æµ‹
        anomalies = await ml_service.detect_anomalies(symbol)
        
        # è·å–å½“å‰ä»·æ ¼
        current_price = await binance_service.get_current_price(symbol)
        
        # æ„å»ºä¼ ç»Ÿä¿¡å·æ•°æ®
        traditional_signals = {
            "supertrend_signals": traditional_analysis.get("supertrend_analysis", {}),
            "current_price": float(current_price) if current_price else None,
            "trend_strength": traditional_analysis.get("trend_strength", "unknown"),
            "support_resistance": traditional_analysis.get("support_resistance", {}),
            "volume_analysis": traditional_analysis.get("volume_analysis", {})
        }
        
        # æ„å»ºMLé¢„æµ‹æ•°æ®
        ml_data = {
            "signal": ml_prediction.signal.value,
            "confidence": ml_prediction.confidence,
            "probability_distribution": ml_prediction.probability_distribution,
            "model_accuracy": ml_prediction.model_accuracy,
            "features_importance": ml_prediction.features_importance or {}
        }
        
        # æ„å»ºå¼‚å¸¸æ£€æµ‹æ•°æ®
        anomaly_data = []
        for anomaly in anomalies:
            anomaly_data.append({
                "type": anomaly.anomaly_type.value,
                "severity": anomaly.severity,
                "description": anomaly.description,
                "recommendation": anomaly.recommendation,
                "timestamp": anomaly.timestamp
            })
        
        # ç»¼åˆåˆ†æç”Ÿæˆå»ºè®®
        combined_recommendation, confidence_score = _generate_combined_recommendation(
            traditional_analysis, ml_prediction, anomalies
        )
        
        return StrategyOverviewResponse(
            status="success",
            symbol=symbol,
            traditional_signals=traditional_signals,
            ml_prediction=ml_data,
            anomaly_detection=anomaly_data,
            combined_recommendation=combined_recommendation,
            confidence_score=confidence_score,
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Strategy overview API failed for {symbol}: {e}")
        raise HTTPException(status_code=500, detail=f"ç­–ç•¥åˆ†æå¤±è´¥: {str(e)}")


def _generate_combined_recommendation(traditional_analysis: Dict, ml_prediction, anomalies: List) -> tuple:
    """
    ç”Ÿæˆç»¼åˆæŠ•èµ„å»ºè®®
    
    Args:
        traditional_analysis: ä¼ ç»ŸæŠ€æœ¯åˆ†æç»“æœ
        ml_prediction: MLé¢„æµ‹ç»“æœ
        anomalies: å¼‚å¸¸æ£€æµ‹ç»“æœ
        
    Returns:
        (å»ºè®®æ–‡æœ¬, ç½®ä¿¡åº¦åˆ†æ•°)
    """
    try:
        # ä½¿ç”¨åŠ¨æ€æƒé‡é…ç½®
        ml_config = get_ml_weight_config()
        traditional_weight = ml_config.get_traditional_weight_for_api()
        ml_weight = ml_config.get_ml_weight_for_api()
        
        logger.debug(f"ğŸ”§ åŠ¨æ€æƒé‡: ä¼ ç»Ÿ={traditional_weight}, ML={ml_weight}, æ¨¡å¼={ml_config.current_mode.value}")
        
        # è·å–ä¼ ç»Ÿä¿¡å·å¼ºåº¦
        traditional_signal = traditional_analysis.get("overall_signal", "hold")
        traditional_strength = traditional_analysis.get("signal_strength", 0.5)
        
        # è·å–MLä¿¡å·å¼ºåº¦
        ml_signal = ml_prediction.signal.value
        ml_confidence = ml_prediction.confidence
        
        # ä¿¡å·æ˜ å°„åˆ°æ•°å€¼ (-2åˆ°2)
        signal_mapping = {
            "strong_sell": -2,
            "sell": -1,
            "hold": 0,
            "buy": 1,
            "strong_buy": 2
        }
        
        traditional_score = signal_mapping.get(traditional_signal, 0) * traditional_strength
        ml_score = signal_mapping.get(ml_signal, 0) * ml_confidence
        
        # è®¡ç®—ç»¼åˆåˆ†æ•°
        combined_score = (traditional_score * traditional_weight + ml_score * ml_weight)
        
        # å¼‚å¸¸æ£€æµ‹è°ƒæ•´
        high_severity_anomalies = [a for a in anomalies if a.severity > 0.7]
        if high_severity_anomalies:
            combined_score *= 0.7  # é™ä½ç½®ä¿¡åº¦
        
        # ç”Ÿæˆå»ºè®®
        if combined_score >= 1.5:
            recommendation = "ğŸš€ å¼ºçƒˆå»ºè®®ä¹°å…¥"
            confidence = min(0.9, abs(combined_score) / 2)
        elif combined_score >= 0.5:
            recommendation = "ğŸ“ˆ å»ºè®®ä¹°å…¥"
            confidence = min(0.8, abs(combined_score) / 2)
        elif combined_score <= -1.5:
            recommendation = "ğŸ’¥ å¼ºçƒˆå»ºè®®å–å‡º"
            confidence = min(0.9, abs(combined_score) / 2)
        elif combined_score <= -0.5:
            recommendation = "ğŸ“‰ å»ºè®®å–å‡º"
            confidence = min(0.8, abs(combined_score) / 2)
        else:
            recommendation = "â¸ï¸ å»ºè®®æŒæœ‰è§‚æœ›"
            confidence = 0.6
        
        # æ·»åŠ è¯¦ç»†è¯´æ˜
        details = []
        details.append(f"ä¼ ç»ŸæŠ€æœ¯æŒ‡æ ‡: {traditional_signal} (å¼ºåº¦: {traditional_strength:.1%})")
        details.append(f"MLé¢„æµ‹ä¿¡å·: {ml_signal} (ç½®ä¿¡åº¦: {ml_confidence:.1%})")
        
        if high_severity_anomalies:
            details.append(f"âš ï¸ æ£€æµ‹åˆ°{len(high_severity_anomalies)}ä¸ªé«˜ä¸¥é‡åº¦å¼‚å¸¸ï¼Œå»ºè®®è°¨æ…æ“ä½œ")
        
        recommendation += "\n\n" + "\n".join(details)
        
        return recommendation, confidence
        
    except Exception as e:
        logger.error(f"Failed to generate combined recommendation: {e}")
        return "âš ï¸ åˆ†æå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•", 0.0


@router.get("/model-status", response_model=BaseResponse)
async def get_model_status() -> BaseResponse:
    """
    è·å–MLæ¨¡å‹çŠ¶æ€
    
    è¿”å›æ‰€æœ‰äº¤æ˜“å¯¹çš„æ¨¡å‹è®­ç»ƒçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
    """
    try:
        model_status = {}
        
        for symbol in settings.monitored_symbols:
            model = ml_service.prediction_models.get(symbol)
            scaler = ml_service.scalers.get(symbol)
            
            if model and scaler:
                accuracy = getattr(model, '_accuracy', 0.0)
                model_status[symbol] = {
                    "model_loaded": True,
                    "accuracy": accuracy,
                    "status": "good" if accuracy >= 0.6 else "needs_improvement" if accuracy >= 0.5 else "poor",
                    "last_trained": "unknown"  # å¯ä»¥ä»æ–‡ä»¶ä¿®æ”¹æ—¶é—´è·å–
                }
            else:
                model_status[symbol] = {
                    "model_loaded": False,
                    "accuracy": 0.0,
                    "status": "not_trained",
                    "last_trained": "never"
                }
        
        return BaseResponse(
            status="success",
            data={
                "models": model_status,
                "total_models": len(model_status),
                "trained_models": sum(1 for s in model_status.values() if s["model_loaded"]),
                "good_performance": sum(1 for s in model_status.values() if s["status"] == "good")
            },
            timestamp=datetime.now()
        )
        
    except Exception as e:
        logger.error(f"Model status API failed: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹çŠ¶æ€å¤±è´¥: {str(e)}")